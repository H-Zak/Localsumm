# .env.example
# Ceci est un fichier d'exemple. Copiez-le en '.env' (qui est ignoré par Git)
# et décommentez/modifiez les lignes nécessaires pour votre configuration locale.

# --- Configuration Générale (Optionnelle - Surcharge les défauts de config.py) ---
# OLLAMA_BASE_URL=http://localhost:11434
# OLLAMA_MODEL=mistral:7b-instruct # Ou llama3:instruct, etc.
# WHISPER_MODEL_SIZE=small # Ou base, medium, etc.
# LOG_LEVEL=INFO # DEBUG, INFO, WARNING, ERROR, CRITICAL

# --- Sélection du Backend de Transcription ---
# Choisissez 'faster-whisper' (défaut si non spécifié) ou 'whisper-cpp'
# TRANSCRIPTION_BACKEND=faster-whisper

# --- Configuration pour le backend 'faster-whisper' (Optionnelle) ---
# FASTER_WHISPER_COMPUTE_TYPE=int8 # Ou float16, etc.
# FASTER_WHISPER_DEVICE=auto # Ou cpu, mps, cuda

# --- Configuration REQUISE si TRANSCRIPTION_BACKEND='whisper-cpp' ---
# Décommentez et fournissez les chemins ABSOLUS corrects sur VOTRE machine.
# WHISPER_CPP_EXECUTABLE_PATH=/chemin/complet/vers/votre/whisper.cpp/build/bin/whisper-cli
# WHISPER_CPP_MODEL_PATH=/chemin/complet/vers/votre/whisper.cpp/models/ggml-medium.bin
# WHISPER_CPP_LANGUAGE=fr # Ou en, auto, etc.
# WHISPER_CPP_THREADS=8 # Nombre de threads CPU à utiliser
